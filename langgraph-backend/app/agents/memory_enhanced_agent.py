"""Example agent enhanced with the new memory management system"""

import logging
from typing import Dict, Any, List
from datetime import datetime

from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_anthropic import ChatAnthropic

from ..state.graph_state import GraphState
from ..memory.memory_persistence import get_memory_node, MemoryPersistenceNode
from ..memory import ContextTier

logger = logging.getLogger(__name__)

class MemoryEnhancedAgent:
    """Example agent that demonstrates memory system integration"""
    
    def __init__(self, agent_type: str = "general"):
        self.agent_type = agent_type
        self.memory_node = get_memory_node()
        self.model = ChatAnthropic(
            model="claude-3-5-sonnet-20241022",
            temperature=0.1
        )
        
    async def process_with_memory(self, state: GraphState) -> GraphState:
        """Process user input with memory-enhanced context"""
        
        try:
            # Ensure memory node is initialized
            await self.memory_node.initialize()
            
            # Step 1: Load relevant memory context
            logger.info(f\"Loading memory context for agent: {self.agent_type}\")\n            enhanced_state = await self.memory_node.load_memory_context(state)\n            \n            # Step 2: Get the current user message\n            current_message = enhanced_state[\"messages\"][-1] if enhanced_state[\"messages\"] else None\n            if not current_message or not isinstance(current_message, HumanMessage):\n                return enhanced_state\n                \n            user_query = current_message.content\n            memory_context = enhanced_state.get(\"memory_context\", [])\n            prompt_fragments = enhanced_state.get(\"prompt_fragments\", [])\n            context_tier = enhanced_state.get(\"context_tier\", \"standard\")\n            \n            # Step 3: Build enhanced system message\n            system_message = self._build_enhanced_system_message(\n                memory_context, prompt_fragments, context_tier\n            )\n            \n            # Step 4: Create context-aware messages for the model\n            model_messages = [system_message]\n            \n            # Add recent conversation history (last 10 messages)\n            recent_messages = enhanced_state[\"messages\"][-10:]\n            model_messages.extend(recent_messages)\n            \n            logger.info(f\"Processing with {len(memory_context)} memories and {len(prompt_fragments)} fragments\")\n            \n            # Step 5: Get AI response\n            response = await self.model.ainvoke(model_messages)\n            \n            # Step 6: Add AI response to state\n            enhanced_state[\"messages\"].append(response)\n            \n            # Step 7: Update metadata\n            if \"metadata\" not in enhanced_state:\n                enhanced_state[\"metadata\"] = {}\n                \n            enhanced_state[\"metadata\"].update({\n                \"agent_type\": self.agent_type,\n                \"memory_enhanced\": True,\n                \"context_tier\": context_tier,\n                \"memories_used\": len(memory_context),\n                \"fragments_used\": len(prompt_fragments),\n                \"consolidated_context\": enhanced_state.get(\"consolidated_context\", False),\n                \"processed_at\": datetime.utcnow().isoformat()\n            })\n            \n            # Step 8: Persist new memories from this conversation\n            final_state = await self.memory_node.persist_conversation_memories(enhanced_state)\n            \n            logger.info(f\"Memory-enhanced processing completed for agent: {self.agent_type}\")\n            \n            return final_state\n            \n        except Exception as e:\n            logger.error(f\"Memory-enhanced processing failed: {e}\")\n            # Fall back to basic processing without memory\n            return await self._fallback_processing(state)\n    \n    def _build_enhanced_system_message(self, memory_context: List[Dict], \n                                     prompt_fragments: List[Dict], \n                                     context_tier: str) -> BaseMessage:\n        \"\"\"Build system message enhanced with memory and fragments\"\"\"\n        \n        base_prompts = {\n            \"general\": \"You are a helpful AI assistant.\",\n            \"sales\": \"You are a sales assistant focused on helping customers find the right products.\",\n            \"products\": \"You are a product management assistant helping with inventory and product information.\",\n            \"pricing\": \"You are a pricing specialist helping with pricing strategies and analysis.\",\n            \"media\": \"You are a media assistant helping with content creation and media management.\",\n            \"integrations\": \"You are an integrations specialist helping with system connections and workflows.\"\n        }\n        \n        system_content = [base_prompts.get(self.agent_type, base_prompts[\"general\"])]\n        \n        # Add memory context\n        if memory_context:\n            system_content.append(\"\\nRELEVANT USER CONTEXT:\")\n            for i, memory in enumerate(memory_context[:5], 1):  # Limit to top 5\n                importance = memory.get(\"importance\", 1.0)\n                category = memory.get(\"category\", \"general\")\n                content = memory.get(\"content\", \"\")\n                system_content.append(f\"{i}. [{category.upper()}] (importance: {importance:.1f}) {content}\")\n        \n        # Add prompt fragments by category\n        if prompt_fragments:\n            fragments_by_category = {}\n            for fragment in prompt_fragments:\n                category = fragment.get(\"category\", \"general\")\n                if category not in fragments_by_category:\n                    fragments_by_category[category] = []\n                fragments_by_category[category].append(fragment[\"content\"])\n            \n            system_content.append(\"\\nADDITIONAL GUIDELINES:\")\n            for category, contents in fragments_by_category.items():\n                system_content.append(f\"\\n{category.upper()}:\")\n                for content in contents:\n                    system_content.append(f\"- {content}\")\n        \n        # Add context tier information\n        tier_instructions = {\n            \"core\": \"Keep responses concise and focus on essential information only.\",\n            \"standard\": \"Provide balanced responses with relevant details and context.\", \n            \"full\": \"Provide comprehensive responses with detailed explanations and examples.\"\n        }\n        \n        if context_tier in tier_instructions:\n            system_content.append(f\"\\nRESPONSE STYLE: {tier_instructions[context_tier]}\")\n        \n        return AIMessage(content=\"\\n\".join(system_content))\n    \n    async def _fallback_processing(self, state: GraphState) -> GraphState:\n        \"\"\"Fallback processing without memory enhancement\"\"\"\n        \n        logger.warning(f\"Using fallback processing for agent: {self.agent_type}\")\n        \n        try:\n            current_message = state[\"messages\"][-1] if state[\"messages\"] else None\n            if not current_message or not isinstance(current_message, HumanMessage):\n                return state\n                \n            # Simple processing with basic system message\n            system_message = AIMessage(content=f\"You are a helpful {self.agent_type} assistant.\")\n            model_messages = [system_message] + state[\"messages\"][-5:]  # Last 5 messages\n            \n            response = await self.model.ainvoke(model_messages)\n            state[\"messages\"].append(response)\n            \n            # Update metadata\n            if \"metadata\" not in state:\n                state[\"metadata\"] = {}\n                \n            state[\"metadata\"].update({\n                \"agent_type\": self.agent_type,\n                \"memory_enhanced\": False,\n                \"fallback_used\": True,\n                \"processed_at\": datetime.utcnow().isoformat()\n            })\n            \n            return state\n            \n        except Exception as e:\n            logger.error(f\"Fallback processing also failed: {e}\")\n            state[\"error\"] = str(e)\n            return state\n\n    async def get_user_memory_summary(self, user_id: str) -> Dict[str, Any]:\n        \"\"\"Get a summary of user's memory profile\"\"\"\n        try:\n            await self.memory_node.initialize()\n            \n            # Get user stats\n            stats = await self.memory_node.get_memory_stats(user_id)\n            \n            # Get recent memories\n            recent_memories = await self.memory_node.search_user_memories(\n                user_id=user_id,\n                query=\"user preferences and important information\",\n                limit=5\n            )\n            \n            return {\n                \"user_id\": user_id,\n                \"stats\": stats,\n                \"recent_memories\": recent_memories,\n                \"agent_type\": self.agent_type\n            }\n            \n        except Exception as e:\n            logger.error(f\"Failed to get user memory summary: {e}\")\n            return {\"error\": str(e)}\n\n# Example usage function for LangGraph integration\nasync def create_memory_enhanced_node(agent_type: str = \"general\"):\n    \"\"\"Factory function to create memory-enhanced agent nodes for LangGraph\"\"\"\n    \n    agent = MemoryEnhancedAgent(agent_type)\n    \n    async def memory_node(state: GraphState) -> GraphState:\n        \"\"\"LangGraph node function with memory enhancement\"\"\"\n        return await agent.process_with_memory(state)\n    \n    return memory_node\n\n# Example of how to use in orchestrator\nasync def example_graph_with_memory():\n    \"\"\"Example of creating a LangGraph with memory-enhanced agents\"\"\"\n    \n    from langgraph.graph import StateGraph\n    from ..state.graph_state import GraphState\n    \n    # Create the graph\n    graph = StateGraph(GraphState)\n    \n    # Add memory-enhanced agents\n    general_agent_node = await create_memory_enhanced_node(\"general\")\n    sales_agent_node = await create_memory_enhanced_node(\"sales\")\n    products_agent_node = await create_memory_enhanced_node(\"products\")\n    \n    # Add nodes to graph\n    graph.add_node(\"general_agent\", general_agent_node)\n    graph.add_node(\"sales_agent\", sales_agent_node)\n    graph.add_node(\"products_agent\", products_agent_node)\n    \n    # Add routing logic (simplified)\n    def route_to_agent(state: GraphState) -> str:\n        # Simple routing based on keywords in the message\n        if not state[\"messages\"]:\n            return \"general_agent\"\n            \n        last_message = state[\"messages\"][-1].content.lower()\n        \n        if any(word in last_message for word in [\"buy\", \"purchase\", \"price\", \"cost\", \"sell\"]):\n            return \"sales_agent\"\n        elif any(word in last_message for word in [\"product\", \"inventory\", \"stock\", \"catalog\"]):\n            return \"products_agent\"\n        else:\n            return \"general_agent\"\n    \n    # Add conditional edges\n    graph.add_conditional_edges(\n        \"__start__\",\n        route_to_agent,\n        {\n            \"general_agent\": \"general_agent\",\n            \"sales_agent\": \"sales_agent\", \n            \"products_agent\": \"products_agent\"\n        }\n    )\n    \n    # Set finish points\n    graph.add_edge(\"general_agent\", \"__end__\")\n    graph.add_edge(\"sales_agent\", \"__end__\")\n    graph.add_edge(\"products_agent\", \"__end__\")\n    \n    return graph.compile()"} ]